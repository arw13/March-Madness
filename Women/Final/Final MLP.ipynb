{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Organization.ipynb\n",
      "Final_Data_Organization.ipynb\n",
      "FinalMarchMadnessAdvStats.csv\n",
      "Final MLP.ipynb\n",
      "MarchMadnessAdvStats.csv\n",
      "MarchMadnessFeatures.csv\n",
      "MarchMadnessTest.csv\n",
      "OldWNCAATourneyCompactResults.csv\n",
      "OldWNCAATourneySeeds.csv\n",
      "wModel.py\n",
      "WNCAATourneyCompactResults.csv\n",
      "WNCAATourneyDetailedResults.csv\n",
      "WNCAATourneySeeds.csv\n",
      "WNCAATourneySlots.csv\n",
      "WRegularSeasonCompactResults.csv\n",
      "WRegularSeasonDetailedResults.csv\n",
      "WSampleSubmissionStage2.csv\n",
      "WSeasons.csv\n",
      "WTeams.csv\n",
      "WTeamSpellings.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras import regularizers, optimizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../Final\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-organized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Final/'\n",
    "df_features = pd.read_csv(data_dir + 'MarchMadnessFeatures.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format and scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector dimension is: 28.00\n"
     ]
    }
   ],
   "source": [
    "X = df_features.iloc[:,1:]\n",
    "xDim = np.shape(X)[1]\n",
    "X_train = X.values.reshape(-1,xDim)\n",
    "y_train = df_features.Result.values\n",
    "\n",
    "print('Feature vector dimension is: %.2f' % xDim)\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "X_train = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(504, 10)\n"
     ]
    }
   ],
   "source": [
    "xDim = np.shape(X_train)[1]\n",
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(data_dir+'MarchMadnessTest.csv')\n",
    "\n",
    "X_test = df_test.iloc[:,1:]\n",
    "xDimTest = np.shape(X_test)[1]\n",
    "X_test = X_test.values.reshape(-1,xDimTest)\n",
    "y_test = df_test.Result.values\n",
    "\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Kfold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropRate = 0.3\n",
    "numBatch = 50\n",
    "numEpoch = 120\n",
    "learningRate = 3e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Single Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "MLP = Sequential()\n",
    "MLP.name = 'MLP'\n",
    "MLP.add(Dense(200, input_dim=xDim, kernel_initializer='glorot_normal',activation = 'tanh'))\n",
    "MLP.add(Dropout(dropRate))\n",
    "# MLP.add(Dense(200,kernel_initializer='glorot_normal',activation = 'tanh'))\n",
    "# MLP.add(Dropout(dropRate))\n",
    "# MLP.add(Dense(200, kernel_initializer='glorot_normal',activation = 'tanh'))\n",
    "# MLP.add(Dropout(dropRate))\n",
    "MLP.add(Dense(1, kernel_initializer='glorot_normal', activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "adam = optimizers.Adam(lr=learningRate, amsgrad=True)\n",
    "MLP.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "504/504 [==============================] - 1s 1ms/step - loss: 0.7116 - acc: 0.6270\n",
      "Epoch 2/120\n",
      "504/504 [==============================] - 0s 52us/step - loss: 0.6862 - acc: 0.6250\n",
      "Epoch 3/120\n",
      "504/504 [==============================] - 0s 59us/step - loss: 0.6621 - acc: 0.6528\n",
      "Epoch 4/120\n",
      "504/504 [==============================] - 0s 59us/step - loss: 0.6560 - acc: 0.6528\n",
      "Epoch 5/120\n",
      "504/504 [==============================] - 0s 57us/step - loss: 0.6679 - acc: 0.6488\n",
      "Epoch 6/120\n",
      "504/504 [==============================] - 0s 51us/step - loss: 0.6545 - acc: 0.6488\n",
      "Epoch 7/120\n",
      "504/504 [==============================] - 0s 46us/step - loss: 0.6493 - acc: 0.6607\n",
      "Epoch 8/120\n",
      "504/504 [==============================] - 0s 59us/step - loss: 0.6280 - acc: 0.6766\n",
      "Epoch 9/120\n",
      "504/504 [==============================] - 0s 48us/step - loss: 0.6319 - acc: 0.6865\n",
      "Epoch 10/120\n",
      "504/504 [==============================] - 0s 63us/step - loss: 0.6063 - acc: 0.6726\n",
      "Epoch 11/120\n",
      "504/504 [==============================] - 0s 61us/step - loss: 0.6207 - acc: 0.6806\n",
      "Epoch 12/120\n",
      "504/504 [==============================] - 0s 46us/step - loss: 0.5972 - acc: 0.6944\n",
      "Epoch 13/120\n",
      "504/504 [==============================] - 0s 56us/step - loss: 0.6006 - acc: 0.6964\n",
      "Epoch 14/120\n",
      "504/504 [==============================] - 0s 70us/step - loss: 0.5984 - acc: 0.6925\n",
      "Epoch 15/120\n",
      "504/504 [==============================] - 0s 56us/step - loss: 0.6135 - acc: 0.6865\n",
      "Epoch 16/120\n",
      "504/504 [==============================] - 0s 53us/step - loss: 0.5879 - acc: 0.6865\n",
      "Epoch 17/120\n",
      "504/504 [==============================] - 0s 46us/step - loss: 0.6018 - acc: 0.6984\n",
      "Epoch 18/120\n",
      "504/504 [==============================] - 0s 54us/step - loss: 0.5691 - acc: 0.7004\n",
      "Epoch 19/120\n",
      "504/504 [==============================] - 0s 60us/step - loss: 0.5854 - acc: 0.6627\n",
      "Epoch 20/120\n",
      "504/504 [==============================] - 0s 56us/step - loss: 0.5535 - acc: 0.7143\n",
      "Epoch 21/120\n",
      "504/504 [==============================] - 0s 53us/step - loss: 0.5538 - acc: 0.7183\n",
      "Epoch 22/120\n",
      "504/504 [==============================] - 0s 64us/step - loss: 0.5795 - acc: 0.6885\n",
      "Epoch 23/120\n",
      "504/504 [==============================] - 0s 60us/step - loss: 0.5511 - acc: 0.7262\n",
      "Epoch 24/120\n",
      "504/504 [==============================] - 0s 53us/step - loss: 0.5609 - acc: 0.7123\n",
      "Epoch 25/120\n",
      "504/504 [==============================] - 0s 52us/step - loss: 0.5635 - acc: 0.7202\n",
      "Epoch 26/120\n",
      "504/504 [==============================] - 0s 48us/step - loss: 0.5674 - acc: 0.7183\n",
      "Epoch 27/120\n",
      "504/504 [==============================] - 0s 55us/step - loss: 0.5292 - acc: 0.7123\n",
      "Epoch 28/120\n",
      "504/504 [==============================] - 0s 48us/step - loss: 0.5510 - acc: 0.7083\n",
      "Epoch 29/120\n",
      "504/504 [==============================] - 0s 58us/step - loss: 0.5373 - acc: 0.7242\n",
      "Epoch 30/120\n",
      "504/504 [==============================] - 0s 56us/step - loss: 0.5407 - acc: 0.7063\n",
      "Epoch 31/120\n",
      "504/504 [==============================] - 0s 48us/step - loss: 0.5405 - acc: 0.7282\n",
      "Epoch 32/120\n",
      "504/504 [==============================] - 0s 60us/step - loss: 0.5345 - acc: 0.7183\n",
      "Epoch 33/120\n",
      "504/504 [==============================] - 0s 57us/step - loss: 0.5597 - acc: 0.7202\n",
      "Epoch 34/120\n",
      "504/504 [==============================] - 0s 46us/step - loss: 0.5123 - acc: 0.7262\n",
      "Epoch 35/120\n",
      "504/504 [==============================] - 0s 61us/step - loss: 0.5532 - acc: 0.7183\n",
      "Epoch 36/120\n",
      "504/504 [==============================] - 0s 59us/step - loss: 0.5438 - acc: 0.7321\n",
      "Epoch 37/120\n",
      "504/504 [==============================] - 0s 66us/step - loss: 0.5330 - acc: 0.7222\n",
      "Epoch 38/120\n",
      "504/504 [==============================] - 0s 63us/step - loss: 0.5079 - acc: 0.7381\n",
      "Epoch 39/120\n",
      "504/504 [==============================] - 0s 48us/step - loss: 0.5246 - acc: 0.7341\n",
      "Epoch 40/120\n",
      "504/504 [==============================] - 0s 48us/step - loss: 0.5198 - acc: 0.7381\n",
      "Epoch 41/120\n",
      "504/504 [==============================] - 0s 58us/step - loss: 0.5257 - acc: 0.7520\n",
      "Epoch 42/120\n",
      "504/504 [==============================] - 0s 48us/step - loss: 0.5156 - acc: 0.7222\n",
      "Epoch 43/120\n",
      "504/504 [==============================] - 0s 53us/step - loss: 0.4912 - acc: 0.7520\n",
      "Epoch 44/120\n",
      "504/504 [==============================] - 0s 67us/step - loss: 0.5159 - acc: 0.7341\n",
      "Epoch 45/120\n",
      "504/504 [==============================] - 0s 56us/step - loss: 0.5089 - acc: 0.7262\n",
      "Epoch 46/120\n",
      "504/504 [==============================] - 0s 67us/step - loss: 0.5097 - acc: 0.7500\n",
      "Epoch 47/120\n",
      "504/504 [==============================] - 0s 47us/step - loss: 0.4929 - acc: 0.7619\n",
      "Epoch 48/120\n",
      "504/504 [==============================] - 0s 63us/step - loss: 0.5115 - acc: 0.7421\n",
      "Epoch 49/120\n",
      "504/504 [==============================] - 0s 63us/step - loss: 0.5233 - acc: 0.7421\n",
      "Epoch 50/120\n",
      "504/504 [==============================] - 0s 49us/step - loss: 0.4997 - acc: 0.7440\n",
      "Epoch 51/120\n",
      "504/504 [==============================] - 0s 57us/step - loss: 0.4974 - acc: 0.7520\n",
      "Epoch 52/120\n",
      "504/504 [==============================] - 0s 46us/step - loss: 0.5129 - acc: 0.7262\n",
      "Epoch 53/120\n",
      "504/504 [==============================] - 0s 58us/step - loss: 0.5000 - acc: 0.7440\n",
      "Epoch 54/120\n",
      "504/504 [==============================] - 0s 49us/step - loss: 0.4939 - acc: 0.7520\n",
      "Epoch 55/120\n",
      "504/504 [==============================] - 0s 60us/step - loss: 0.4858 - acc: 0.7579\n",
      "Epoch 56/120\n",
      "504/504 [==============================] - 0s 51us/step - loss: 0.5024 - acc: 0.7302\n",
      "Epoch 57/120\n",
      "504/504 [==============================] - 0s 52us/step - loss: 0.4852 - acc: 0.7540\n",
      "Epoch 58/120\n",
      "504/504 [==============================] - 0s 47us/step - loss: 0.4882 - acc: 0.7599\n",
      "Epoch 59/120\n",
      "504/504 [==============================] - 0s 53us/step - loss: 0.4940 - acc: 0.7540\n",
      "Epoch 60/120\n",
      "504/504 [==============================] - 0s 51us/step - loss: 0.5008 - acc: 0.7440\n",
      "Epoch 61/120\n",
      "504/504 [==============================] - 0s 73us/step - loss: 0.4912 - acc: 0.7659\n",
      "Epoch 62/120\n",
      "504/504 [==============================] - 0s 60us/step - loss: 0.4780 - acc: 0.7401\n",
      "Epoch 63/120\n",
      "504/504 [==============================] - 0s 75us/step - loss: 0.4808 - acc: 0.7599\n",
      "Epoch 64/120\n",
      "504/504 [==============================] - 0s 61us/step - loss: 0.4870 - acc: 0.7500\n",
      "Epoch 65/120\n",
      "504/504 [==============================] - 0s 47us/step - loss: 0.4891 - acc: 0.7440\n",
      "Epoch 66/120\n",
      "504/504 [==============================] - 0s 51us/step - loss: 0.4730 - acc: 0.7520\n",
      "Epoch 67/120\n",
      "504/504 [==============================] - 0s 46us/step - loss: 0.4927 - acc: 0.7480\n",
      "Epoch 68/120\n",
      "504/504 [==============================] - 0s 59us/step - loss: 0.4732 - acc: 0.7679\n",
      "Epoch 69/120\n",
      "504/504 [==============================] - 0s 53us/step - loss: 0.4825 - acc: 0.7639\n",
      "Epoch 70/120\n",
      "504/504 [==============================] - 0s 56us/step - loss: 0.4906 - acc: 0.7480\n",
      "Epoch 71/120\n",
      "504/504 [==============================] - 0s 64us/step - loss: 0.4849 - acc: 0.7560\n",
      "Epoch 72/120\n",
      "504/504 [==============================] - 0s 66us/step - loss: 0.4782 - acc: 0.7718\n",
      "Epoch 73/120\n",
      "504/504 [==============================] - 0s 65us/step - loss: 0.4781 - acc: 0.7619\n",
      "Epoch 74/120\n",
      "504/504 [==============================] - 0s 63us/step - loss: 0.4875 - acc: 0.7520\n",
      "Epoch 75/120\n",
      "504/504 [==============================] - 0s 66us/step - loss: 0.4877 - acc: 0.7520\n",
      "Epoch 76/120\n",
      "504/504 [==============================] - 0s 64us/step - loss: 0.4852 - acc: 0.7798\n",
      "Epoch 77/120\n",
      "504/504 [==============================] - 0s 51us/step - loss: 0.4700 - acc: 0.7738\n",
      "Epoch 78/120\n",
      "504/504 [==============================] - 0s 61us/step - loss: 0.4836 - acc: 0.7520\n",
      "Epoch 79/120\n",
      "504/504 [==============================] - 0s 50us/step - loss: 0.4828 - acc: 0.7520\n",
      "Epoch 80/120\n",
      "504/504 [==============================] - 0s 66us/step - loss: 0.4723 - acc: 0.7579\n",
      "Epoch 81/120\n",
      "504/504 [==============================] - 0s 52us/step - loss: 0.4714 - acc: 0.7718\n",
      "Epoch 82/120\n",
      "504/504 [==============================] - 0s 64us/step - loss: 0.4768 - acc: 0.7679\n",
      "Epoch 83/120\n",
      "504/504 [==============================] - 0s 58us/step - loss: 0.4976 - acc: 0.7302\n",
      "Epoch 84/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 0s 65us/step - loss: 0.4735 - acc: 0.7659\n",
      "Epoch 85/120\n",
      "504/504 [==============================] - 0s 52us/step - loss: 0.4710 - acc: 0.7540\n",
      "Epoch 86/120\n",
      "504/504 [==============================] - 0s 58us/step - loss: 0.4611 - acc: 0.7679\n",
      "Epoch 87/120\n",
      "504/504 [==============================] - 0s 52us/step - loss: 0.4771 - acc: 0.7659\n",
      "Epoch 88/120\n",
      "504/504 [==============================] - 0s 53us/step - loss: 0.4631 - acc: 0.7679\n",
      "Epoch 89/120\n",
      "504/504 [==============================] - 0s 48us/step - loss: 0.4708 - acc: 0.7599\n",
      "Epoch 90/120\n",
      "504/504 [==============================] - 0s 48us/step - loss: 0.4716 - acc: 0.7639\n",
      "Epoch 91/120\n",
      "504/504 [==============================] - 0s 53us/step - loss: 0.4791 - acc: 0.7540\n",
      "Epoch 92/120\n",
      "504/504 [==============================] - 0s 46us/step - loss: 0.4620 - acc: 0.7599\n",
      "Epoch 93/120\n",
      "504/504 [==============================] - 0s 48us/step - loss: 0.4611 - acc: 0.7698\n",
      "Epoch 94/120\n",
      "504/504 [==============================] - 0s 50us/step - loss: 0.4608 - acc: 0.7817\n",
      "Epoch 95/120\n",
      "504/504 [==============================] - 0s 46us/step - loss: 0.4656 - acc: 0.7619\n",
      "Epoch 96/120\n",
      "504/504 [==============================] - 0s 48us/step - loss: 0.4602 - acc: 0.7599\n",
      "Epoch 97/120\n",
      "504/504 [==============================] - 0s 52us/step - loss: 0.4721 - acc: 0.7500\n",
      "Epoch 98/120\n",
      "504/504 [==============================] - 0s 45us/step - loss: 0.4760 - acc: 0.7639\n",
      "Epoch 99/120\n",
      "504/504 [==============================] - 0s 46us/step - loss: 0.4675 - acc: 0.7738\n",
      "Epoch 100/120\n",
      "504/504 [==============================] - 0s 51us/step - loss: 0.4536 - acc: 0.7738\n",
      "Epoch 101/120\n",
      "504/504 [==============================] - 0s 50us/step - loss: 0.4690 - acc: 0.7698\n",
      "Epoch 102/120\n",
      "504/504 [==============================] - 0s 48us/step - loss: 0.4769 - acc: 0.7560\n",
      "Epoch 103/120\n",
      "504/504 [==============================] - 0s 49us/step - loss: 0.4734 - acc: 0.7778\n",
      "Epoch 104/120\n",
      "504/504 [==============================] - 0s 45us/step - loss: 0.4630 - acc: 0.7639\n",
      "Epoch 105/120\n",
      "504/504 [==============================] - 0s 55us/step - loss: 0.4735 - acc: 0.7560\n",
      "Epoch 106/120\n",
      "504/504 [==============================] - 0s 46us/step - loss: 0.4751 - acc: 0.7599\n",
      "Epoch 107/120\n",
      "504/504 [==============================] - 0s 49us/step - loss: 0.4657 - acc: 0.7778\n",
      "Epoch 108/120\n",
      "504/504 [==============================] - 0s 56us/step - loss: 0.4698 - acc: 0.7540\n",
      "Epoch 109/120\n",
      "504/504 [==============================] - 0s 55us/step - loss: 0.4550 - acc: 0.7659\n",
      "Epoch 110/120\n",
      "504/504 [==============================] - 0s 67us/step - loss: 0.4588 - acc: 0.7659\n",
      "Epoch 111/120\n",
      "504/504 [==============================] - 0s 55us/step - loss: 0.4607 - acc: 0.7659\n",
      "Epoch 112/120\n",
      "504/504 [==============================] - 0s 57us/step - loss: 0.4696 - acc: 0.7798\n",
      "Epoch 113/120\n",
      "504/504 [==============================] - 0s 64us/step - loss: 0.4649 - acc: 0.7877\n",
      "Epoch 114/120\n",
      "504/504 [==============================] - 0s 51us/step - loss: 0.4433 - acc: 0.7956\n",
      "Epoch 115/120\n",
      "504/504 [==============================] - 0s 51us/step - loss: 0.4523 - acc: 0.7679\n",
      "Epoch 116/120\n",
      "504/504 [==============================] - 0s 58us/step - loss: 0.4809 - acc: 0.7560\n",
      "Epoch 117/120\n",
      "504/504 [==============================] - 0s 55us/step - loss: 0.4610 - acc: 0.7738\n",
      "Epoch 118/120\n",
      "504/504 [==============================] - 0s 61us/step - loss: 0.4555 - acc: 0.7837\n",
      "Epoch 119/120\n",
      "504/504 [==============================] - 0s 53us/step - loss: 0.4470 - acc: 0.7798\n",
      "Epoch 120/120\n",
      "504/504 [==============================] - 0s 56us/step - loss: 0.4595 - acc: 0.7619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f227bc4be48>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.fit(X_train, y_train, epochs=numEpoch, batch_size=numBatch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 826us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47323935183267746, 0.7751322760783806]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.3816087166876445\n",
      "Validation set score: 0.4837797427646516\n"
     ]
    }
   ],
   "source": [
    "fore = RandomForestClassifier(criterion='entropy', n_estimators = 50, max_depth = 4, oob_score=True)\n",
    "\n",
    "fore.fit(X_train, y_train)\n",
    "\n",
    "y_pred =  fore.predict_proba(X_train)[:,1].reshape(-1,1)\n",
    "LL = log_loss( y_train, y_pred)\n",
    "print(\"Training set score: {:4}\" .format(LL))\n",
    "y_pred =  fore.predict_proba(X_test)[:,1].reshape(-1,1)\n",
    "LL = log_loss( y_test, y_pred)\n",
    "print(\"Validation set score: {:4}\".format(LL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7671957671957672\n"
     ]
    }
   ],
   "source": [
    "accTest = fore.score(X_test, y_test)\n",
    "print(\"Validation accuracy: {:4}\".format(accTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Final/'\n",
    "df_sample_sub = pd.read_csv(data_dir + 'WSampleSubmissionStage2.csv')\n",
    "data_file = data_dir + 'FinalMarchMadnessAdvStats.csv'\n",
    "df_adv = pd.read_csv(data_file)\n",
    "df_seeds = pd.read_csv(data_dir + 'WNCAATourneySeeds.csv')\n",
    "\n",
    "\n",
    "n_test_games = len(df_sample_sub)\n",
    "\n",
    "def get_year_t1_t2(ID):\n",
    "    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n",
    "    return (int(x) for x in ID.split('_'))\n",
    "\n",
    "def seed_to_int(seed):\n",
    "    '''Get just the digits from the seeding. Return as int'''\n",
    "    s_int = int(seed[1:3])\n",
    "    return s_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for submission test\n"
     ]
    }
   ],
   "source": [
    "print('Loading data for submission test')\n",
    "\n",
    "# Make the seeding an integer\n",
    "df_seeds['seed_int'] = df_seeds.Seed.apply(seed_to_int)\n",
    "df_seeds.drop(columns=['Seed'], inplace=True) # This is the string label\n",
    "df_seeds.head()\n",
    "\n",
    "\n",
    "T1_seed = []\n",
    "T1_adv = []\n",
    "T2_adv = []\n",
    "T2_seed = []\n",
    "for ii, row in df_sample_sub.iterrows():\n",
    "    year, t1, t2 = get_year_t1_t2(row.ID)\n",
    "    t1_seed = df_seeds[(df_seeds.TeamID == t1) & (df_seeds.Season == year)].seed_int.values[0]\n",
    "    t2_seed = df_seeds[(df_seeds.TeamID == t2) & (df_seeds.Season == year)].seed_int.values[0]\n",
    "    t1_adv = df_adv[(df_adv.TeamID == t1) & (df_adv.Season == year)].values[0]\n",
    "    t2_adv = df_adv[(df_adv.TeamID == t2) & (df_adv.Season == year)].values[0]\n",
    "    T1_seed.append(t1_seed)\n",
    "    T1_adv.append(t1_adv)\n",
    "    T2_seed.append(t2_seed)\n",
    "    T2_adv.append(t2_adv)\n",
    "\n",
    "T1_adv = [row[2:] for row in T1_adv]\n",
    "T2_adv = [row[2:] for row in T2_adv]\n",
    "T1_seed = np.reshape(T1_seed, [n_test_games,-1]).tolist()\n",
    "T2_seed = np.reshape(T2_seed, [n_test_games, -1]).tolist()\n",
    "X_pred = np.concatenate((T1_seed, T1_adv, T2_seed, T2_adv), axis=1)\n",
    "\n",
    "df_subData = pd.DataFrame(np.array(X_pred).reshape(np.shape(X_pred)[0], np.shape(X_pred)[1]))\n",
    "\n",
    "xDim = np.shape(df_subData)[1]\n",
    "X_pred = df_subData.values.reshape(-1,xDim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2016, 28)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = pca.transform(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2016, 2)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = MLP.predict_proba(X_pred)\n",
    "\n",
    "# df_sample_sub = pd.DataFrame()\n",
    "# clipped_preds = np.clip(preds, 0.05, 0.95)\n",
    "df_sample_sub.Pred = preds\n",
    "df_sample_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2016, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fore.predict_proba(X_pred)\n",
    "\n",
    "# df_sample_sub = pd.DataFrame()\n",
    "# clipped_preds = np.clip(preds, 0.05, 0.95)\n",
    "df_sample_sub.Pred = preds\n",
    "df_sample_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'MLP2'\n",
    "save_dir = '../Final/'\n",
    "c=0\n",
    "ext = '.csv'\n",
    "if os.path.exists(save_dir+filename+ext):\n",
    "    while os.path.exists(filename+ext):\n",
    "        c+=1\n",
    "    filename = filename+'_'+str(c)\n",
    "    df_sample_sub.to_csv(save_dir+filename+ext, index=False)\n",
    "else:\n",
    "    df_sample_sub.to_csv(save_dir+filename+ext, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
