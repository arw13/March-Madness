{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "Data_Organization.ipynb\n",
      "Final\n",
      "Final_Data_Organization.ipynb\n",
      "Final MLP.ipynb\n",
      "GiantKillerInfo.txt\n",
      "logs\n",
      "MarchMadnessAdvStats.csv\n",
      "MarchMadnessFeatureDifferences.csv\n",
      "MarchMadnessFeatures.csv\n",
      "MarchMadnessTest.csv\n",
      "MLP Make Bracket.ipynb\n",
      "ModelEvaluation.ipynb\n",
      "OldNbs\n",
      "README.md\n",
      "SubmissionData.py\n",
      "Submissions\n",
      "tENSORFLOW.ipynb\n",
      "tENSORFLOW.py\n",
      "tENSORFLOW-TB.ipynb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras import regularizers, optimizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../March-Madness\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-organized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../March-Madness/'\n",
    "df_features = pd.read_csv(data_dir + 'MarchMadnessFeatures.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format and scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector dimension is: 34.00\n"
     ]
    }
   ],
   "source": [
    "X = df_features.iloc[:,1:]\n",
    "xDim = np.shape(X)[1]\n",
    "X_train = X.values.reshape(-1,xDim)\n",
    "y_train = df_features.Result.values\n",
    "\n",
    "print('Feature vector dimension is: %.2f' % xDim)\n",
    "# print(X[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "xDim = np.shape(X_train)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(data_dir+'MarchMadnessTest.csv')\n",
    "\n",
    "X_test = df_test.iloc[:,1:]\n",
    "xDimTest = np.shape(X_test)[1]\n",
    "X_test = X_test.values.reshape(-1,xDimTest)\n",
    "y_test = df_test.Result.values\n",
    "\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Kfold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropRate = 0.3\n",
    "numBatch = 20\n",
    "numEpoch = 120\n",
    "learningRate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Single Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "MLP = Sequential()\n",
    "MLP.name = 'MLP'\n",
    "MLP.add(Dense(5, input_dim=xDim, kernel_initializer='glorot_normal',activation = 'tanh'))\n",
    "MLP.add(Dropout(dropRate))\n",
    "MLP.add(Dense(200,kernel_initializer='glorot_normal',activation = 'tanh'))\n",
    "MLP.add(Dropout(dropRate))\n",
    "MLP.add(Dense(200, kernel_initializer='glorot_normal',activation = 'tanh'))\n",
    "MLP.add(Dropout(dropRate))\n",
    "MLP.add(Dense(1, kernel_initializer='glorot_normal', activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "adam = optimizers.Adam(lr=learningRate, amsgrad=True)\n",
    "MLP.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "530/530 [==============================] - 1s 1ms/step - loss: 0.7038 - acc: 0.4962\n",
      "Epoch 2/120\n",
      "530/530 [==============================] - 0s 57us/step - loss: 0.7023 - acc: 0.5226\n",
      "Epoch 3/120\n",
      "530/530 [==============================] - 0s 58us/step - loss: 0.7053 - acc: 0.5000\n",
      "Epoch 4/120\n",
      "530/530 [==============================] - 0s 53us/step - loss: 0.7045 - acc: 0.4755\n",
      "Epoch 5/120\n",
      "530/530 [==============================] - 0s 61us/step - loss: 0.7101 - acc: 0.4774\n",
      "Epoch 6/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6977 - acc: 0.5151\n",
      "Epoch 7/120\n",
      "530/530 [==============================] - 0s 57us/step - loss: 0.7005 - acc: 0.5094\n",
      "Epoch 8/120\n",
      "530/530 [==============================] - 0s 55us/step - loss: 0.6941 - acc: 0.5170\n",
      "Epoch 9/120\n",
      "530/530 [==============================] - 0s 50us/step - loss: 0.7060 - acc: 0.4792\n",
      "Epoch 10/120\n",
      "530/530 [==============================] - 0s 56us/step - loss: 0.6821 - acc: 0.5491\n",
      "Epoch 11/120\n",
      "530/530 [==============================] - 0s 58us/step - loss: 0.6929 - acc: 0.5226\n",
      "Epoch 12/120\n",
      "530/530 [==============================] - 0s 60us/step - loss: 0.7015 - acc: 0.5094\n",
      "Epoch 13/120\n",
      "530/530 [==============================] - 0s 63us/step - loss: 0.6856 - acc: 0.5604\n",
      "Epoch 14/120\n",
      "530/530 [==============================] - 0s 55us/step - loss: 0.6985 - acc: 0.5264\n",
      "Epoch 15/120\n",
      "530/530 [==============================] - 0s 52us/step - loss: 0.6864 - acc: 0.5547\n",
      "Epoch 16/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6908 - acc: 0.5264\n",
      "Epoch 17/120\n",
      "530/530 [==============================] - 0s 61us/step - loss: 0.6892 - acc: 0.5604\n",
      "Epoch 18/120\n",
      "530/530 [==============================] - 0s 58us/step - loss: 0.6887 - acc: 0.5302\n",
      "Epoch 19/120\n",
      "530/530 [==============================] - 0s 59us/step - loss: 0.6827 - acc: 0.5340\n",
      "Epoch 20/120\n",
      "530/530 [==============================] - 0s 57us/step - loss: 0.6879 - acc: 0.5491\n",
      "Epoch 21/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6813 - acc: 0.5604\n",
      "Epoch 22/120\n",
      "530/530 [==============================] - 0s 51us/step - loss: 0.6865 - acc: 0.5623\n",
      "Epoch 23/120\n",
      "530/530 [==============================] - 0s 58us/step - loss: 0.6816 - acc: 0.5528\n",
      "Epoch 24/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6941 - acc: 0.5415\n",
      "Epoch 25/120\n",
      "530/530 [==============================] - 0s 56us/step - loss: 0.6855 - acc: 0.5396\n",
      "Epoch 26/120\n",
      "530/530 [==============================] - 0s 55us/step - loss: 0.6904 - acc: 0.5340\n",
      "Epoch 27/120\n",
      "530/530 [==============================] - 0s 59us/step - loss: 0.6762 - acc: 0.6019\n",
      "Epoch 28/120\n",
      "530/530 [==============================] - 0s 53us/step - loss: 0.6771 - acc: 0.5623\n",
      "Epoch 29/120\n",
      "530/530 [==============================] - 0s 55us/step - loss: 0.6821 - acc: 0.5509\n",
      "Epoch 30/120\n",
      "530/530 [==============================] - 0s 52us/step - loss: 0.6787 - acc: 0.5792\n",
      "Epoch 31/120\n",
      "530/530 [==============================] - 0s 56us/step - loss: 0.6767 - acc: 0.5792\n",
      "Epoch 32/120\n",
      "530/530 [==============================] - 0s 59us/step - loss: 0.6792 - acc: 0.5604\n",
      "Epoch 33/120\n",
      "530/530 [==============================] - 0s 74us/step - loss: 0.6792 - acc: 0.5396\n",
      "Epoch 34/120\n",
      "530/530 [==============================] - 0s 57us/step - loss: 0.6817 - acc: 0.5698\n",
      "Epoch 35/120\n",
      "530/530 [==============================] - 0s 55us/step - loss: 0.6798 - acc: 0.5472\n",
      "Epoch 36/120\n",
      "530/530 [==============================] - 0s 58us/step - loss: 0.6744 - acc: 0.5792\n",
      "Epoch 37/120\n",
      "530/530 [==============================] - 0s 58us/step - loss: 0.6787 - acc: 0.5660\n",
      "Epoch 38/120\n",
      "530/530 [==============================] - 0s 62us/step - loss: 0.6849 - acc: 0.5434\n",
      "Epoch 39/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6801 - acc: 0.5679\n",
      "Epoch 40/120\n",
      "530/530 [==============================] - 0s 57us/step - loss: 0.6796 - acc: 0.5736\n",
      "Epoch 41/120\n",
      "530/530 [==============================] - 0s 52us/step - loss: 0.6830 - acc: 0.5358\n",
      "Epoch 42/120\n",
      "530/530 [==============================] - 0s 64us/step - loss: 0.6757 - acc: 0.5811\n",
      "Epoch 43/120\n",
      "530/530 [==============================] - 0s 52us/step - loss: 0.6802 - acc: 0.5792\n",
      "Epoch 44/120\n",
      "530/530 [==============================] - 0s 55us/step - loss: 0.6701 - acc: 0.5943\n",
      "Epoch 45/120\n",
      "530/530 [==============================] - 0s 57us/step - loss: 0.6696 - acc: 0.6264\n",
      "Epoch 46/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6739 - acc: 0.6019\n",
      "Epoch 47/120\n",
      "530/530 [==============================] - 0s 50us/step - loss: 0.6777 - acc: 0.5943\n",
      "Epoch 48/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6708 - acc: 0.5925\n",
      "Epoch 49/120\n",
      "530/530 [==============================] - 0s 50us/step - loss: 0.6713 - acc: 0.5830\n",
      "Epoch 50/120\n",
      "530/530 [==============================] - 0s 53us/step - loss: 0.6744 - acc: 0.5811\n",
      "Epoch 51/120\n",
      "530/530 [==============================] - 0s 61us/step - loss: 0.6732 - acc: 0.5868\n",
      "Epoch 52/120\n",
      "530/530 [==============================] - 0s 60us/step - loss: 0.6789 - acc: 0.5679\n",
      "Epoch 53/120\n",
      "530/530 [==============================] - 0s 60us/step - loss: 0.6653 - acc: 0.6132\n",
      "Epoch 54/120\n",
      "530/530 [==============================] - 0s 55us/step - loss: 0.6708 - acc: 0.6019\n",
      "Epoch 55/120\n",
      "530/530 [==============================] - 0s 47us/step - loss: 0.6799 - acc: 0.5736\n",
      "Epoch 56/120\n",
      "530/530 [==============================] - 0s 53us/step - loss: 0.6672 - acc: 0.6019\n",
      "Epoch 57/120\n",
      "530/530 [==============================] - 0s 50us/step - loss: 0.6702 - acc: 0.5962\n",
      "Epoch 58/120\n",
      "530/530 [==============================] - 0s 49us/step - loss: 0.6671 - acc: 0.6302\n",
      "Epoch 59/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6799 - acc: 0.5736\n",
      "Epoch 60/120\n",
      "530/530 [==============================] - 0s 52us/step - loss: 0.6681 - acc: 0.6019\n",
      "Epoch 61/120\n",
      "530/530 [==============================] - 0s 48us/step - loss: 0.6725 - acc: 0.5660\n",
      "Epoch 62/120\n",
      "530/530 [==============================] - 0s 55us/step - loss: 0.6713 - acc: 0.5906\n",
      "Epoch 63/120\n",
      "530/530 [==============================] - 0s 56us/step - loss: 0.6655 - acc: 0.6189\n",
      "Epoch 64/120\n",
      "530/530 [==============================] - 0s 49us/step - loss: 0.6599 - acc: 0.6245\n",
      "Epoch 65/120\n",
      "530/530 [==============================] - 0s 50us/step - loss: 0.6679 - acc: 0.5925\n",
      "Epoch 66/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6649 - acc: 0.6000\n",
      "Epoch 67/120\n",
      "530/530 [==============================] - 0s 51us/step - loss: 0.6686 - acc: 0.5660\n",
      "Epoch 68/120\n",
      "530/530 [==============================] - 0s 50us/step - loss: 0.6623 - acc: 0.5981\n",
      "Epoch 69/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6542 - acc: 0.6302\n",
      "Epoch 70/120\n",
      "530/530 [==============================] - 0s 57us/step - loss: 0.6696 - acc: 0.5698\n",
      "Epoch 71/120\n",
      "530/530 [==============================] - 0s 47us/step - loss: 0.6695 - acc: 0.5698\n",
      "Epoch 72/120\n",
      "530/530 [==============================] - 0s 52us/step - loss: 0.6574 - acc: 0.6189\n",
      "Epoch 73/120\n",
      "530/530 [==============================] - 0s 49us/step - loss: 0.6691 - acc: 0.5868\n",
      "Epoch 74/120\n",
      "530/530 [==============================] - 0s 58us/step - loss: 0.6574 - acc: 0.6113\n",
      "Epoch 75/120\n",
      "530/530 [==============================] - 0s 58us/step - loss: 0.6561 - acc: 0.6208\n",
      "Epoch 76/120\n",
      "530/530 [==============================] - 0s 58us/step - loss: 0.6683 - acc: 0.5906\n",
      "Epoch 77/120\n",
      "530/530 [==============================] - 0s 67us/step - loss: 0.6534 - acc: 0.6208\n",
      "Epoch 78/120\n",
      "530/530 [==============================] - 0s 55us/step - loss: 0.6631 - acc: 0.6170\n",
      "Epoch 79/120\n",
      "530/530 [==============================] - 0s 53us/step - loss: 0.6677 - acc: 0.6000\n",
      "Epoch 80/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6574 - acc: 0.6132\n",
      "Epoch 81/120\n",
      "530/530 [==============================] - 0s 47us/step - loss: 0.6608 - acc: 0.6057\n",
      "Epoch 82/120\n",
      "530/530 [==============================] - 0s 59us/step - loss: 0.6574 - acc: 0.6094\n",
      "Epoch 83/120\n",
      "530/530 [==============================] - 0s 51us/step - loss: 0.6642 - acc: 0.6057\n",
      "Epoch 84/120\n",
      "530/530 [==============================] - 0s 48us/step - loss: 0.6596 - acc: 0.6208\n",
      "Epoch 85/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6625 - acc: 0.6057\n",
      "Epoch 86/120\n",
      "530/530 [==============================] - 0s 48us/step - loss: 0.6596 - acc: 0.6245\n",
      "Epoch 87/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6638 - acc: 0.6208\n",
      "Epoch 88/120\n",
      "530/530 [==============================] - 0s 58us/step - loss: 0.6560 - acc: 0.6283\n",
      "Epoch 89/120\n",
      "530/530 [==============================] - 0s 53us/step - loss: 0.6558 - acc: 0.6113\n",
      "Epoch 90/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6546 - acc: 0.6283\n",
      "Epoch 91/120\n",
      "530/530 [==============================] - 0s 53us/step - loss: 0.6593 - acc: 0.6302\n",
      "Epoch 92/120\n",
      "530/530 [==============================] - 0s 51us/step - loss: 0.6516 - acc: 0.6302\n",
      "Epoch 93/120\n",
      "530/530 [==============================] - 0s 49us/step - loss: 0.6538 - acc: 0.6245\n",
      "Epoch 94/120\n",
      "530/530 [==============================] - 0s 46us/step - loss: 0.6675 - acc: 0.6113\n",
      "Epoch 95/120\n",
      "530/530 [==============================] - 0s 49us/step - loss: 0.6567 - acc: 0.6019\n",
      "Epoch 96/120\n",
      "530/530 [==============================] - 0s 47us/step - loss: 0.6511 - acc: 0.6245\n",
      "Epoch 97/120\n",
      "530/530 [==============================] - 0s 55us/step - loss: 0.6617 - acc: 0.6170\n",
      "Epoch 98/120\n",
      "530/530 [==============================] - 0s 62us/step - loss: 0.6548 - acc: 0.6226\n",
      "Epoch 99/120\n",
      "530/530 [==============================] - 0s 49us/step - loss: 0.6487 - acc: 0.6377\n",
      "Epoch 100/120\n",
      "530/530 [==============================] - 0s 48us/step - loss: 0.6669 - acc: 0.5962\n",
      "Epoch 101/120\n",
      "530/530 [==============================] - 0s 53us/step - loss: 0.6458 - acc: 0.6340\n",
      "Epoch 102/120\n",
      "530/530 [==============================] - 0s 51us/step - loss: 0.6580 - acc: 0.6226\n",
      "Epoch 103/120\n",
      "530/530 [==============================] - 0s 51us/step - loss: 0.6586 - acc: 0.6019\n",
      "Epoch 104/120\n",
      "530/530 [==============================] - 0s 53us/step - loss: 0.6512 - acc: 0.6283\n",
      "Epoch 105/120\n",
      "530/530 [==============================] - 0s 58us/step - loss: 0.6593 - acc: 0.6226\n",
      "Epoch 106/120\n",
      "530/530 [==============================] - 0s 55us/step - loss: 0.6534 - acc: 0.6075\n",
      "Epoch 107/120\n",
      "530/530 [==============================] - 0s 52us/step - loss: 0.6455 - acc: 0.6491\n",
      "Epoch 108/120\n",
      "530/530 [==============================] - 0s 50us/step - loss: 0.6620 - acc: 0.5981\n",
      "Epoch 109/120\n",
      "530/530 [==============================] - 0s 63us/step - loss: 0.6578 - acc: 0.6245\n",
      "Epoch 110/120\n",
      "530/530 [==============================] - 0s 61us/step - loss: 0.6422 - acc: 0.6358\n",
      "Epoch 111/120\n",
      "530/530 [==============================] - 0s 54us/step - loss: 0.6531 - acc: 0.6472\n",
      "Epoch 112/120\n",
      "530/530 [==============================] - 0s 59us/step - loss: 0.6520 - acc: 0.6434\n",
      "Epoch 113/120\n",
      "530/530 [==============================] - 0s 71us/step - loss: 0.6483 - acc: 0.6321\n",
      "Epoch 114/120\n",
      "530/530 [==============================] - 0s 68us/step - loss: 0.6559 - acc: 0.6132\n",
      "Epoch 115/120\n",
      "530/530 [==============================] - 0s 53us/step - loss: 0.6656 - acc: 0.6075\n",
      "Epoch 116/120\n",
      "530/530 [==============================] - 0s 57us/step - loss: 0.6554 - acc: 0.6094\n",
      "Epoch 117/120\n",
      "530/530 [==============================] - 0s 57us/step - loss: 0.6454 - acc: 0.6472\n",
      "Epoch 118/120\n",
      "530/530 [==============================] - 0s 56us/step - loss: 0.6491 - acc: 0.6170\n",
      "Epoch 119/120\n",
      "530/530 [==============================] - 0s 50us/step - loss: 0.6521 - acc: 0.6321\n",
      "Epoch 120/120\n",
      "530/530 [==============================] - 0s 49us/step - loss: 0.6448 - acc: 0.6604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c3ba09be0>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.fit(X_train, y_train, epochs=numEpoch, batch_size=numBatch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402/402 [==============================] - 0s 661us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6227560594900331, 0.6815920400234005]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fore = RandomForestClassifier(criterion='entropy', n_estimators = 50, max_depth = 4, oob_score=True)\n",
    "\n",
    "fore.fit(X_train, y_train)\n",
    "\n",
    "y_pred =  fore.predict_proba(X_train)[:,1].reshape(-1,1)\n",
    "LL = log_loss( y_train, y_pred)\n",
    "print(\"Training set score: {:4}\" .format(LL))\n",
    "y_pred =  fore.predict_proba(X_test)[:,1].reshape(-1,1)\n",
    "LL = log_loss( y_test, y_pred)\n",
    "print(\"Validation set score: {:4}\".format(LL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.736318407960199\n"
     ]
    }
   ],
   "source": [
    "accTest = fore.score(X_test, y_test)\n",
    "print(\"Validation accuracy: {:4}\".format(accTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../March-Madness/Final/'\n",
    "df_sample_sub = pd.read_csv(data_dir + 'SampleSubmissionStage2.csv')\n",
    "data_file = data_dir + 'MarchMadnessAdvStats.csv'\n",
    "df_adv = pd.read_csv(data_file)\n",
    "df_seeds = pd.read_csv(data_dir + 'NCAATourneySeeds.csv')\n",
    "\n",
    "\n",
    "n_test_games = len(df_sample_sub)\n",
    "\n",
    "def get_year_t1_t2(ID):\n",
    "    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n",
    "    return (int(x) for x in ID.split('_'))\n",
    "\n",
    "def seed_to_int(seed):\n",
    "    '''Get just the digits from the seeding. Return as int'''\n",
    "    s_int = int(seed[1:3])\n",
    "    return s_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for submission test\n"
     ]
    }
   ],
   "source": [
    "print('Loading data for submission test')\n",
    "\n",
    "# Make the seeding an integer\n",
    "df_seeds['seed_int'] = df_seeds.Seed.apply(seed_to_int)\n",
    "df_seeds.drop(columns=['Seed'], inplace=True) # This is the string label\n",
    "df_seeds.head()\n",
    "\n",
    "\n",
    "T1_seed = []\n",
    "T1_adv = []\n",
    "T2_adv = []\n",
    "T2_seed = []\n",
    "for ii, row in df_sample_sub.iterrows():\n",
    "    year, t1, t2 = get_year_t1_t2(row.ID)\n",
    "    t1_seed = df_seeds[(df_seeds.TeamID == t1) & (df_seeds.Season == year)].seed_int.values[0]\n",
    "    t2_seed = df_seeds[(df_seeds.TeamID == t2) & (df_seeds.Season == year)].seed_int.values[0]\n",
    "    t1_adv = df_adv[(df_adv.TeamID == t1) & (df_adv.Season == year)].values[0]\n",
    "    t2_adv = df_adv[(df_adv.TeamID == t2) & (df_adv.Season == year)].values[0]\n",
    "    T1_seed.append(t1_seed)\n",
    "    T1_adv.append(t1_adv)\n",
    "    T2_seed.append(t2_seed)\n",
    "    T2_adv.append(t2_adv)\n",
    "\n",
    "T1_adv = [row[2:] for row in T1_adv]\n",
    "T2_adv = [row[2:] for row in T2_adv]\n",
    "T1_seed = np.reshape(T1_seed, [n_test_games,-1]).tolist()\n",
    "T2_seed = np.reshape(T2_seed, [n_test_games, -1]).tolist()\n",
    "X_pred = np.concatenate((T1_seed, T1_adv, T2_seed, T2_adv), axis=1)\n",
    "\n",
    "df_subData = pd.DataFrame(np.array(X_pred).reshape(np.shape(X_pred)[0], np.shape(X_pred)[1]))\n",
    "\n",
    "xDim = np.shape(df_subData)[1]\n",
    "X_pred = df_subData.values.reshape(-1,xDim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2278, 34)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = pca.transform(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2278, 2)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = MLP.predict_proba(X_pred)\n",
    "\n",
    "# df_sample_sub = pd.DataFrame()\n",
    "# clipped_preds = np.clip(preds, 0.05, 0.95)\n",
    "df_sample_sub.Pred = preds\n",
    "df_sample_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2278, 2)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fore.predict_proba(X_pred)\n",
    "\n",
    "# df_sample_sub = pd.DataFrame()\n",
    "# clipped_preds = np.clip(preds, 0.05, 0.95)\n",
    "df_sample_sub.Pred = preds\n",
    "df_sample_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'RF'\n",
    "save_dir = '../March-Madness/Final/'\n",
    "c=0\n",
    "ext = '.csv'\n",
    "if os.path.exists(save_dir+filename+ext):\n",
    "    while os.path.exists(filename+ext):\n",
    "        c+=1\n",
    "    filename = filename+'_'+str(c)\n",
    "    df_sample_sub.to_csv(save_dir+filename+ext, index=False)\n",
    "else:\n",
    "    df_sample_sub.to_csv(save_dir+filename+ext, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
